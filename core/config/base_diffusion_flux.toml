# ULTRA AGGRESSIVE H100×4 FLUX CONFIGURATION FOR RANK 1 PERFORMANCE
# This configuration pushes H100×4 hardware to its absolute limits

[general]
pretrained_model_name_or_path = "stabilityai/stable-diffusion-xl-base-1.0"
train_data_dir = "/dataset/images"
output_dir = "/workspace/axolotl/outputs"
huggingface_token = ""
huggingface_repo_id = ""

[training]
# ULTRA AGGRESSIVE BATCH SIZES FOR H100×4
train_batch_size = 8  # 4x larger for H100×4
vae_batch_size = 32  # 4x larger VAE batch

# ULTRA AGGRESSIVE LoRA SETTINGS FOR RANK 1
network_dim = 512  # 16x larger LoRA rank for maximum expressiveness
network_alpha = 512  # 16x larger alpha for better convergence

# ULTRA AGGRESSIVE TRAINING PARAMETERS
max_train_steps = 12000  # 200% more training for better convergence
epoch = 600  # 200% more epochs for better convergence
learning_rate = 2.0e-4  # 150% higher LR for faster convergence
text_encoder_lr = [2.0e-4, 2.0e-4]
unet_lr = 2.0e-4

# ULTRA AGGRESSIVE OPTIMIZATION SETTINGS
lr_scheduler = "cosine_with_restarts"
lr_scheduler_args = [8]  # More cycles
lr_scheduler_num_cycles = 8  # More cycles for better convergence
min_snr_gamma = 16  # Better noise scheduling
huber_c = 0.02  # More stable loss
max_grad_norm = 0.2  # Tighter gradient clipping
scale_weight_norms = 32  # Higher scale for better training

# ULTRA AGGRESSIVE DATA LOADING
max_data_loader_n_workers = 32  # More workers for H100×4
dataloader_pin_memory = true
dataloader_num_workers = 32

# ULTRA AGGRESSIVE SAVING AND LOGGING
save_every_n_epochs = 2  # More frequent saving
save_steps = 200  # More frequent saving
logging_steps = 10  # More frequent logging
eval_steps = 100  # More frequent evaluation

# ULTRA AGGRESSIVE MEMORY OPTIMIZATIONS
mixed_precision = "bf16"  # Use BF16 for H100×4
use_8bit_adam = false  # Not needed with H100×4
use_4bit_adam = false  # Not needed with H100×4
gradient_checkpointing = false  # Disable for H100×4 (enough memory)

# ULTRA AGGRESSIVE NETWORK SETTINGS
network_weights = ""
network_dropout = 0.02  # Lower dropout for better performance
network_args = ["conv_dim=1", "conv_alpha=1", "use_timestep"]
network_show_meta = true

# ULTRA AGGRESSIVE OPTIMIZER SETTINGS
optimizer_type = "AdamW8bit"
optimizer_args = ["weight_decay=0.001", "betas=0.9,0.999", "eps=1e-8"]
lr_scheduler_args = ["num_cycles=8", "power=1.0"]

# ULTRA AGGRESSIVE TRAINING ARGS
max_grad_norm = 0.2
max_train_steps = 12000
save_steps = 200
save_total_limit = 10
logging_steps = 10
eval_steps = 100
evaluation_strategy = "steps"
prediction_loss_only = true
dataloader_num_workers = 32
remove_unused_columns = false
label_names = ["labels"]
load_best_model_at_end = true
metric_for_best_model = "eval_loss"
greater_is_better = false

# ULTRA AGGRESSIVE MEMORY SETTINGS
max_memory_MB = 75000  # 75GB per H100 GPU (pushing limits)
gradient_clipping = 0.2  # Tighter gradient clipping for rank 1

# ULTRA AGGRESSIVE PEFT SETTINGS
use_peft = true
peft_config = {
    "peft_type": "LORA",
    "task_type": "CAUSAL_LM",
    "inference_mode": false,
    "r": 512,
    "lora_alpha": 512,
    "lora_dropout": 0.02,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "bias": "none",
    "modules_to_save": null,
}

# ULTRA AGGRESSIVE FSDP SETTINGS
fsdp = "full_shard auto_wrap"
fsdp_config = {
    "fsdp_auto_wrap_policy": "TRANSFORMER_BASED_WRAP",
    "fsdp_backward_prefetch": "BACKWARD_PRE",
    "fsdp_state_dict_type": "FULL_STATE_DICT",
    "fsdp_transformer_layer_cls_to_wrap": "LlamaDecoderLayer",
    "fsdp_use_orig_params": true,
    "fsdp_forward_prefetch": true,
    "fsdp_limit_all_gathers": true,
}

# ULTRA AGGRESSIVE TRAINING COMMENT
training_comment = "H100×4 ULTRA AGGRESSIVE FLUX for RANK 1 PERFORMANCE"